{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizado de Máquina Trabalho 1\n",
    "\n",
    "Estudantes: Vitor Hugo Garcez, Gabriel Velloso, Bruno Battesini, Larissa Esteves e Nicolas Pietro\n",
    "\n",
    "# Introdução\n",
    "___\n",
    "Este trabalho se propõe a avaliar os resultados alcançados pelo modelo de linguagem ChatGPT, desenvolvido pela OpenAI, ao criar algoritmos para três dos métodos de aprendizado supervisionado amplamente estudados em aula: k-Nearest Neighbors (kNN), Naïve Bayes e Árvores de Decisão. Nosso objetivo principal é analisar não apenas a eficácia dos algoritmos desenvolvidos pelo ChatGPT, mas também a adequação dos prompts utilizados para interagir com o modelo e as escolhas de projeto feitas pelo próprio modelo.\n",
    "\n",
    "Nossa abordagem enfatiza a implementação \"from scratch\" desses algoritmos, desafiando o ChatGPT a criar implementações sem depender de bibliotecas com implementações prontas, como o scikit-learn. Para realizar esse experimento, utilizaremos bibliotecas auxiliares, como o NumPy, para computação vetorial, garantindo que o processo de construção dos algoritmos seja controlado e transparente.\n",
    "\n",
    "É importante destacar que, além de desenvolver os algoritmos, este trabalho também avaliará a capacidade do ChatGPT em criar prompts adequados para gerar resultados precisos e úteis. A análise crítica dos resultados gerados pelo ChatGPT desempenha um papel fundamental, permitindo-nos identificar pontos fortes, potenciais problemas e diferenças em relação às implementações discutidas em sala de aula. Além disso, exploraremos as escolhas de projeto feitas pelo ChatGPT e seu impacto na implementação dos algoritmos, oferecendo sugestões de melhorias e refinamentos.\n",
    "\n",
    "# Desenvolvimento\n",
    "___\n",
    "Esta seção aborda a construção dos três modelos de aprendizado de máquina e sua performance. Para garantir uma comparação justa, optamos por utilizar o mesmo conjunto de dados referente aos pinguins. O objetivo dos modelos é aprender os atributos e classificar o sexo dos pinguins. Todos os prompts utilizados na criação dos algoritmos foram formulados em língua inglesa, uma vez que é sabido que o ChatGPT desenvolvido pela OpenAI interpreta melhor os comandos nesse idioma.\n",
    "\n",
    "## _Dataset_\n",
    "\n",
    "O grupo optou por utilizar um conjunto de dados sobre pinguins do Kaggle devido à sua descrição clara e à presença de uma distribuição equilibrada de 50% para ambos os atributos alvo: _FEMALE_ (fêmea) e _MALE_ (macho). O conjunto de dados inclui os seguintes atributos: espécie, comprimento do culmen, profundidade do culmen, comprimento da nadadeira, massa corporal, ilha de residência e sexo. Na etapa de engenharia de atributos, optamos por mapear as variáveis categóricas, como sexo, ilha e espécie, para os valores 0, 1 e 2, a fim de permitir que os modelos lidem apenas com variáveis numéricas, eliminando a necessidade de qualquer tipo de tratamento adicional.\n",
    "\n",
    "Para facilitar a separação do conjunto de dados entre treino e teste, solicitamos ao ChatGPT a implementação do método holdout usando o comando _`Write a train_test_split function`_. A implementação exigiu uma adaptação para o pandas dataframe com o comando _`I'm using a pandas dataframe`_, uma vez que o uso dessa biblioteca simplifica a leitura e a manipulação dos dados para o grupo.\n",
    "\n",
    "## Métricas\n",
    "As métricas utilizadas para a comparação dos modelos desenvolvidos pelo ChatGPT foram acurácia, recall, precisão e pontuação F1. Os métodos foram implementados a partir do comando _`Without using scikit-learn, write a way to calculate accuracy, recall, precision, and F1 score`_, e não foi necessário realizar nenhuma alteração adicional, pois as implementações funcionaram bem com todos os modelos.\n",
    "\n",
    "## _K-Nearest Neighbors_\n",
    "\n",
    "O algoritmo k-Nearest Neighbors (KNN) foi desenvolvido a partir do comando _`Without using scikit-learn, write a KNN algorithm`_. O ChatGPT foi capaz de gerar quase que integralmente uma classe para o KNN. A única modificação necessária foi uma adaptação para tornar os métodos públicos _predict_ e privado _ _predict_ compatíveis com o pandas dataframe, uma vez que essa biblioteca facilita a manipulação de dados para o grupo. A implementação foi realizada com a função de distância euclidiana, sem a opção de utilizar outras como a gaussiana ou a distância do cosseno.\n",
    "\n",
    "## Naïve Bayes\n",
    "\n",
    "O algoritmo Naïve Bayes foi desenvolvido a partir do comando _`Write a Naïve Bayes algorithm without using scikit-learn`_. Curiosamente, o ChatGPT não criou uma classe para a implementação, em vez disso, foram desenvolvidos todos os métodos separadamente. O ChatGPT manteve o mesmo padrão de desenvolvimento mencionado na seção _K-Nearest Neighbors_ e optou por utilizar apenas uma função de cálculo fixa para o modelo, que foi a gaussiana. Da mesma forma que na seção anterior, foi necessário solicitar que as funções fossem adaptadas para suportar o pandas dataframe.\n",
    "\n",
    "## Árvores de Decisão\n",
    "\n",
    "A implementação da árvore de decisão foi considerada a mais complexa, pois envolve métodos recursivos durante o treinamento. Devido à complexidade da classe, foi necessário refazer diversas vezes os métodos _fit, predict_, e _predict_single_ para que conseguíssemos executar o programa. A classe foi gerada a partir do comando _`Write a decision tree classifier without using scikit-learn`_, e também foi necessário adaptar todos os métodos para que pudessem suportar o pandas dataframe.\n",
    "\n",
    "# Conclusão\n",
    "___\n",
    "\n",
    "Em resumo, este experimento demonstrou que o ChatGPT foi capaz de implementar com sucesso os modelos _K Nearest Neighbors, Gaussian Naïve Bayes e Decision Tree_. Embora a implementação inicial não tenha sido perfeita, com algumas correções foi possível fazer com que todos os modelos alcançassem uma performance superior a 70%. A tabela abaixo apresenta uma comparação de resultados entre os modelos _K Nearest Neighbors, Gaussian Naïve Bayes e Decision Tree_:\n",
    "\n",
    "|_Model_|_Accuracy_|_Precision_|_recall_|_F1 Score_|\n",
    "|:-----:|:---------|:---------:|:------:|:--------:|\n",
    "|_K-Nearest Neighbors_|0.82|0.85|0.81|0.83|\n",
    "|_Gaussian Naive Bayes_|0.70|0.74|0.69|0.71|\n",
    "|_Decision Tree_|0.83|0.86|0.83|0.85|\n",
    "\n",
    "Observamos que, para o conjunto de dados em questão, o melhor desempenho foi obtido com o modelo de Árvore de Decisão. Isso se deve ao fato de que as árvores de decisão podem capturar relações não lineares nos dados de forma mais eficaz do que o Naive Bayes, que assume independência condicional entre os atributos, e são robustas em relação a outliers e dados ruidosos, uma vez que a divisão dos nós é baseada em limiares e frequências relativas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#                   imports                   #\n",
    "###############################################\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#                   Dataset                   #\n",
    "###############################################\n",
    "\n",
    "# Read the CSV file into a DataFrame: df\n",
    "df = pd.read_csv('penguins_size.csv')\n",
    "\n",
    "# Define the mapping\n",
    "species_mapping = {'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}\n",
    "island_mapping = {'Torgersen': 0, 'Biscoe': 1, 'Dream': 2}\n",
    "sex_mapping = {'MALE': 0, 'FEMALE': 1}\n",
    "\n",
    "# Map the values in the 'species' column\n",
    "df['species'] = df['species'].map(species_mapping)\n",
    "df['island'] = df['island'].map(island_mapping)\n",
    "df['sex'] = df['sex'].map(sex_mapping)\n",
    "\n",
    "# Removing NaN values\n",
    "mask = df['sex'].isin([0, 1])\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_train, X_test, y_train, y_test = train_test_split_df(df, target_column=\\'sex\\', test_size=0.2, random_seed=42)\\naccuracy = calculate_accuracy(y_true, y_pred)\\nprecision = calculate_precision(y_true, y_pred, positive_class=1)\\nrecall = calculate_recall(y_true, y_pred, positive_class=1)\\nf1_score = calculate_f1_score(y_true, y_pred, positive_class=1)\\n\\nprint(f\"Accuracy: {accuracy}\")\\nprint(f\"Precision: {precision}\")\\nprint(f\"Recall: {recall}\")\\nprint(f\"F1 Score: {f1_score}\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################\n",
    "#              Metrics and Hodout             #\n",
    "###############################################\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true.values == y_pred)\n",
    "    total_predictions = len(y_true)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "def calculate_precision(y_true, y_pred, positive_class):\n",
    "    true_positives = np.sum((y_true.values == positive_class) & (y_pred == positive_class))\n",
    "    false_positives = np.sum((y_true.values != positive_class) & (y_pred == positive_class))\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    return precision\n",
    "\n",
    "def calculate_recall(y_true, y_pred, positive_class):\n",
    "    true_positives = np.sum((y_true.values == positive_class) & (y_pred == positive_class))\n",
    "    false_negatives = np.sum((y_true.values == positive_class) & (y_pred != positive_class))\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    return recall\n",
    "\n",
    "def calculate_f1_score(y_true, y_pred, positive_class):\n",
    "    precision = calculate_precision(y_true, y_pred, positive_class)\n",
    "    recall = calculate_recall(y_true, y_pred, positive_class)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "def train_test_split_df(df, target_column, test_size=0.3, random_seed=None):\n",
    "    \"\"\"\n",
    "    Split a Pandas DataFrame into training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The Pandas DataFrame containing your data.\n",
    "    - target_column: The name of the target column in the DataFrame.\n",
    "    - test_size: The proportion of the dataset to include in the test split (default is 0.3).\n",
    "    - random_seed: Seed for random number generation (optional).\n",
    "\n",
    "    Returns:\n",
    "    - X_train: The training feature DataFrame.\n",
    "    - X_test: The testing feature DataFrame.\n",
    "    - y_train: The training target Series.\n",
    "    - y_test: The testing target Series.\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    num_samples = len(df)\n",
    "    num_test_samples = int(test_size * num_samples)\n",
    "\n",
    "    # Shuffle the indices to randomize the data split\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "\n",
    "    # Split the data\n",
    "    test_indices = shuffled_indices[:num_test_samples]\n",
    "    train_indices = shuffled_indices[num_test_samples:]\n",
    "\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    X_train = X.iloc[train_indices]\n",
    "    X_test = X.iloc[test_indices]\n",
    "    y_train = y.iloc[train_indices]\n",
    "    y_test = y.iloc[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split_df(df, target_column='sex', test_size=0.2, random_seed=42)\n",
    "accuracy = calculate_accuracy(y_true, y_pred)\n",
    "precision = calculate_precision(y_true, y_pred, positive_class=1)\n",
    "recall = calculate_recall(y_true, y_pred, positive_class=1)\n",
    "f1_score = calculate_f1_score(y_true, y_pred, positive_class=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#             K-Nearest Neighbors             #\n",
    "###############################################\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X.values]\n",
    "        return pd.Series(y_pred, index=X.index)  # Return predictions as a Pandas Series\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [self.euclidean_distance(x, x_train) for x_train in self.X_train.values]\n",
    "        # Sort by distance and return indices of the first k neighbors\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        # Extract the labels of the k nearest neighbor training samples\n",
    "        k_nearest_labels = [self.y_train.iloc[i] for i in k_indices]\n",
    "        # Return the most common class label\n",
    "        most_common = pd.Series(k_nearest_labels).mode().values[0]  # Handle ties\n",
    "        return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#            Gaussian Naive Bayes             #\n",
    "###############################################\n",
    "\n",
    "def calculate_mean_std(X):\n",
    "    # Calculate mean and standard deviation for each feature in X\n",
    "    means = X.mean(axis=0)\n",
    "    stds = X.std(axis=0)\n",
    "    \n",
    "    return means, stds\n",
    "\n",
    "def gaussian_probability(x, mean, std):\n",
    "    # Calculate the Gaussian probability density function\n",
    "    exponent = np.exp(-((x - mean) ** 2) / (2 * std ** 2))\n",
    "    return (1 / (std * np.sqrt(2 * np.pi))) * exponent\n",
    "\n",
    "def train_naive_bayes(X_train, y_train):\n",
    "    # Calculate class priors\n",
    "    unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "    priors = class_counts / len(y_train)\n",
    "    \n",
    "    # Convert DataFrame to NumPy array\n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values\n",
    "    \n",
    "    # Calculate mean and standard deviation for each feature and class\n",
    "    num_classes = len(unique_classes)\n",
    "    num_features = X_train.shape[1]\n",
    "    means = np.zeros((num_classes, num_features))\n",
    "    stds = np.zeros((num_classes, num_features))\n",
    "    \n",
    "    for i, class_label in enumerate(unique_classes):\n",
    "        class_data = X_train[y_train == class_label]\n",
    "        means[i, :], stds[i, :] = calculate_mean_std(class_data)\n",
    "    \n",
    "    return priors, means, stds\n",
    "\n",
    "def predict_naive_bayes(X_test, priors, means, stds):\n",
    "    # Convert DataFrame to NumPy array\n",
    "    X_test = X_test.values\n",
    "    \n",
    "    num_classes = len(priors)\n",
    "    num_samples = X_test.shape[0]\n",
    "    predictions = np.zeros((num_samples, num_classes))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_classes):\n",
    "            class_prior = np.log(priors[j])\n",
    "            likelihood = np.sum(np.log(gaussian_probability(X_test[i, :], means[j, :], stds[j, :])))\n",
    "            predictions[i, j] = class_prior + likelihood\n",
    "    \n",
    "    return np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#                Decision Tree                #\n",
    "###############################################\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        # Check termination conditions\n",
    "        if depth == self.max_depth or len(set(y)) == 1:\n",
    "            return {'class': np.argmax(np.bincount(y))}\n",
    "        \n",
    "        # Find the best split\n",
    "        feature, threshold = self.find_best_split(X, y)\n",
    "\n",
    "        if feature is None:\n",
    "            return {'class': np.argmax(np.bincount(y))}\n",
    "\n",
    "        # Split the data\n",
    "        left_mask = X[feature] <= threshold\n",
    "        right_mask = X[feature] > threshold\n",
    "\n",
    "        # Recursively build left and right subtrees\n",
    "        left_subtree = self.fit(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self.fit(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        self.tree = {'feature': feature, 'threshold': threshold, 'left': left_subtree, 'right': right_subtree}\n",
    "        return self.tree  # Return the constructed tree\n",
    "\n",
    "    def find_best_split(self, X, y):\n",
    "        m, n = X.shape\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "\n",
    "        num_classes = len(set(y))\n",
    "        if num_classes == 1:\n",
    "            return None, None\n",
    "\n",
    "        gini_parent = 1.0 - sum((np.sum(y == c) / m) ** 2 for c in set(y))\n",
    "\n",
    "        best_gini = 1.0\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in X.columns:\n",
    "            thresholds = sorted(X[feature].unique())\n",
    "\n",
    "            for i in range(1, len(thresholds)):\n",
    "                threshold = (thresholds[i - 1] + thresholds[i]) / 2\n",
    "                left_mask = X[feature] <= threshold\n",
    "                right_mask = X[feature] > threshold\n",
    "\n",
    "                if sum(left_mask) == 0 or sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                gini_left = 1.0 - sum((np.sum(y[left_mask] == c) / sum(left_mask)) ** 2 for c in set(y))\n",
    "                gini_right = 1.0 - sum((np.sum(y[right_mask] == c) / sum(right_mask)) ** 2 for c in set(y))\n",
    "\n",
    "                weighted_gini = (sum(left_mask) / m) * gini_left + (sum(right_mask) / m) * gini_right\n",
    "\n",
    "                if weighted_gini < best_gini:\n",
    "                    best_gini = weighted_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for _, row in X.iterrows():\n",
    "            predictions.append(self.predict_single(self.tree, row))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def predict_single(self, tree, x):\n",
    "        if 'class' in tree:\n",
    "            return tree['class']\n",
    "\n",
    "        feature = tree.get('feature')\n",
    "        if feature is None:\n",
    "            # Handle the case where a leaf node is reached without a 'feature' key\n",
    "            return tree.get('class')\n",
    "\n",
    "        threshold = tree['threshold']\n",
    "        left_subtree = tree['left']\n",
    "        right_subtree = tree['right']\n",
    "\n",
    "        if x[feature] <= threshold:\n",
    "            return self.predict_single(left_subtree, x)\n",
    "        else:\n",
    "            return self.predict_single(right_subtree, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "Accuracy: 0.82\n",
      "Precision: 0.85\n",
      "Recall: 0.81\n",
      "F1 Score: 0.83\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "Accuracy: 0.70\n",
      "Precision: 0.74\n",
      "Recall: 0.69\n",
      "F1 Score: 0.71\n",
      "\n",
      "Decision Tree Classifier\n",
      "Accuracy: 0.83\n",
      "Precision: 0.86\n",
      "Recall: 0.83\n",
      "F1 Score: 0.85\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "#            Apprentices Committee            #\n",
    "###############################################\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split_df(df, 'sex', test_size=0.2, random_seed=42)\n",
    "\n",
    "############# K-Nearest Neighbors #############\n",
    "\n",
    "# Create KNN classifier\n",
    "knn = KNN(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "predictions = knn.predict(X_test)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = calculate_accuracy(y_test, predictions)\n",
    "precision = calculate_precision(y_test, predictions, positive_class=1)\n",
    "recall = calculate_recall(y_test, predictions, positive_class=1)\n",
    "f1_score = calculate_f1_score(y_test, predictions, positive_class=1)\n",
    "\n",
    "print('K-Nearest Neighbors')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "############ Gaussian Naive Bayes #############\n",
    "\n",
    "priors, means, stds = train_naive_bayes(X_train, y_train)\n",
    "y_pred = predict_naive_bayes(X_test, priors, means, stds)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "accuracy = calculate_accuracy(y_test, y_pred)\n",
    "precision = calculate_precision(y_test, y_pred, positive_class=1)\n",
    "recall = calculate_recall(y_test, y_pred, positive_class=1)\n",
    "f1_score = calculate_f1_score(y_test, y_pred, positive_class=1)\n",
    "\n",
    "print('\\nGaussian Naive Bayes')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "\n",
    "############### Decision Tree #################\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "accuracy = calculate_accuracy(y_test, y_pred)\n",
    "precision = calculate_precision(y_test, y_pred, positive_class=1)\n",
    "recall = calculate_recall(y_test, y_pred, positive_class=1)\n",
    "f1_score = calculate_f1_score(y_test, y_pred, positive_class=1)\n",
    "\n",
    "print('\\nDecision Tree Classifier')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
